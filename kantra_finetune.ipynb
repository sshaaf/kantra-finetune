{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ ROBUST Phi-3 Mini Fine-tuning for Kantra Rules Generation\n",
        "\n",
        "## üéØ **New Robust Pipeline Features:**\n",
        "\n",
        "### **‚úÖ Key Improvements:**\n",
        "1. **Explicit Chat Template Formatting** - Ensures consistent user/assistant structure\n",
        "2. **System Prompt Integration** - Forces YAML-only output during training\n",
        "3. **Automatic Model Merging** - Creates standalone model (no PEFT complexity)\n",
        "4. **Simple Inference** - Load and use like any regular model\n",
        "5. **Better Error Handling** - Robust validation and fallbacks\n",
        "\n",
        "### **üîÑ Pipeline Overview:**\n",
        "```\n",
        "Training Data ‚Üí Explicit Formatting ‚Üí LoRA Fine-tuning ‚Üí Model Merging ‚Üí Standalone Model\n",
        "```\n",
        "\n",
        "### **üìã Expected Results:**\n",
        "- ‚úÖ **Consistent YAML Output** (no conversational text)\n",
        "- ‚úÖ **Simple Inference** (no adapter loading)\n",
        "- ‚úÖ **Better Reliability** (merged model is more stable)\n",
        "- ‚úÖ **Easy Deployment** (single model directory)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ Phi-3 Mini Fine-tuning for Kantra Rules Generation\n",
        "\n",
        "This notebook fine-tunes Microsoft's Phi-3-mini-4k-instruct model to generate Kantra migration rules using LoRA (Low-Rank Adaptation) for parameter-efficient training.\n",
        "\n",
        "## üìã What this notebook does:\n",
        "- Fine-tunes Phi-3-mini (3.8B params) using QLoRA\n",
        "- Uses your custom Kantra rules dataset\n",
        "- Optimized for Google Colab (free GPU)\n",
        "- Creates a specialized model for migration rule generation\n",
        "\n",
        "## ‚ö° Expected Performance:\n",
        "- **Colab Free (T4)**: ~15-30 minutes training time\n",
        "- **Colab Pro (A100)**: ~5-15 minutes training time\n",
        "- **Local Mac**: ~2.5-4.5 hours (not recommended)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚ö†Ô∏è Step 1: GPU Check\n",
        "\n",
        "**IMPORTANT**: Make sure you're using a GPU runtime before proceeding!\n",
        "\n",
        "Go to: **Runtime** ‚Üí **Change runtime type** ‚Üí **Hardware accelerator** ‚Üí **T4 GPU**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '.venv (Python 3.10.17)' requires the ipykernel package.\n",
            "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/Users/sshaaf/git/finetune/kantra-finetune/.venv/bin/python -m pip install ipykernel -U --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# Verify GPU is available\n",
        "import torch\n",
        "\n",
        "try:\n",
        "    assert torch.cuda.is_available() is True\n",
        "    print(\"‚úÖ GPU is available!\")\n",
        "    print(f\"üöÄ Using GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "except AssertionError:\n",
        "    print(\"‚ùå GPU is not available!\")\n",
        "    print(\"‚ö†Ô∏è Please set up a GPU before using this notebook:\")\n",
        "    print(\"   1. Go to Runtime ‚Üí Change runtime type\")\n",
        "    print(\"   2. Select 'T4 GPU' under Hardware accelerator\")\n",
        "    print(\"   3. Click Save and restart the runtime\")\n",
        "    print(\"   4. Re-run this cell\")\n",
        "    raise RuntimeError(\"GPU required for efficient fine-tuning. Please enable GPU and restart.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì• Step 2: Clone Repository\n",
        "\n",
        "Clone the Kantra fine-tuning repository and install dependencies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone the repository and set up environment\n",
        "%cd /content/\n",
        "%rm -rf kantra-finetune\n",
        "!git clone --depth 1 https://github.com/sshaaf/kantra-finetune.git\n",
        "%cd kantra-finetune\n",
        "%ls\n",
        "\n",
        "# Install PyTorch with CUDA support first\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "# Install bitsandbytes separately (common Colab issue)\n",
        "!pip install bitsandbytes\n",
        "\n",
        "# Install core fine-tuning packages\n",
        "!pip install transformers datasets accelerate peft trl\n",
        "\n",
        "# Install flash attention (optional, may fail but that's OK)\n",
        "!pip install flash-attn --no-build-isolation || echo \"‚ö†Ô∏è Flash attention install failed - will use eager attention\"\n",
        "\n",
        "# Install any additional dependencies from the repository (if setup.py exists)\n",
        "!pip install -e . || echo \"‚ÑπÔ∏è No setup.py found - using manual package installation\"\n",
        "\n",
        "print(\"‚úÖ Installation completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Alternative Installation (Run if above fails)\n",
        "\n",
        "If the installation above fails, run this cell for a more robust installation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Alternative installation method\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_package(package):\n",
        "    \"\"\"Install package with error handling\"\"\"\n",
        "    try:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "        print(f\"‚úÖ {package} installed successfully\")\n",
        "        return True\n",
        "    except subprocess.CalledProcessError:\n",
        "        print(f\"‚ùå Failed to install {package}\")\n",
        "        return False\n",
        "\n",
        "# Essential packages\n",
        "packages = [\n",
        "    \"torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\",\n",
        "    \"transformers\",\n",
        "    \"datasets\", \n",
        "    \"accelerate\",\n",
        "    \"peft\",\n",
        "    \"trl\",\n",
        "    \"psutil\"  # For memory checking\n",
        "]\n",
        "\n",
        "print(\"üîß Installing packages individually...\")\n",
        "for package in packages:\n",
        "    install_package(package)\n",
        "\n",
        "# Try bitsandbytes with different approaches\n",
        "print(\"\\nüîß Installing bitsandbytes...\")\n",
        "bitsandbytes_installed = False\n",
        "\n",
        "# Method 1: Standard pip install\n",
        "if install_package(\"bitsandbytes\"):\n",
        "    bitsandbytes_installed = True\n",
        "else:\n",
        "    # Method 2: Try with --no-deps\n",
        "    print(\"üîÑ Trying bitsandbytes with --no-deps...\")\n",
        "    try:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"bitsandbytes\", \"--no-deps\"])\n",
        "        bitsandbytes_installed = True\n",
        "        print(\"‚úÖ bitsandbytes installed with --no-deps\")\n",
        "    except:\n",
        "        print(\"‚ùå bitsandbytes installation failed\")\n",
        "\n",
        "# Optional packages\n",
        "print(\"\\nüîß Installing optional packages...\")\n",
        "optional_packages = [\"flash-attn\"]\n",
        "for package in optional_packages:\n",
        "    if not install_package(f\"{package} --no-build-isolation\"):\n",
        "        print(f\"‚ö†Ô∏è {package} failed - will use fallback\")\n",
        "\n",
        "print(f\"\\n‚úÖ Installation complete!\")\n",
        "print(f\"üì¶ Quantization available: {bitsandbytes_installed}\")\n",
        "\n",
        "if not bitsandbytes_installed:\n",
        "    print(\"‚ö†Ô∏è Warning: bitsandbytes not installed - will run without quantization\")\n",
        "    print(\"üí° This will use more memory but should still work\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Step 3: Environment Setup\n",
        "\n",
        "Verify the installation and check hardware configuration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify installation and setup\n",
        "import torch\n",
        "import sys\n",
        "import os\n",
        "\n",
        "print(f\"üìç Current directory: {os.getcwd()}\")\n",
        "print(f\"üêç Python version: {sys.version}\")\n",
        "print(f\"üî• PyTorch version: {torch.__version__}\")\n",
        "print(f\"‚ö° CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üöÄ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"‚ùå No GPU available - this will be very slow!\")\n",
        "\n",
        "# Check if bitsandbytes is available\n",
        "try:\n",
        "    import bitsandbytes\n",
        "    bitsandbytes_available = True\n",
        "    print(f\"üì¶ BitsAndBytes version: {bitsandbytes.__version__}\")\n",
        "except ImportError:\n",
        "    bitsandbytes_available = False\n",
        "    print(\"‚ö†Ô∏è BitsAndBytes not available - will run without quantization\")\n",
        "\n",
        "# Set device configuration\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "use_quantization = torch.cuda.is_available() and bitsandbytes_available\n",
        "\n",
        "print(f\"\\n‚úÖ Configuration:\")\n",
        "print(f\"   üéØ Device: {device}\")\n",
        "print(f\"   üì¶ Quantization: {use_quantization}\")\n",
        "print(f\"   üìÅ Working directory: {os.getcwd()}\")\n",
        "\n",
        "if not use_quantization and torch.cuda.is_available():\n",
        "    print(\"   ‚ö†Ô∏è GPU available but quantization disabled (no bitsandbytes)\")\n",
        "    print(\"   üí° Will use more GPU memory but should still work\")\n",
        "\n",
        "# Verify key files exist\n",
        "if os.path.exists(\"train_dataset.jsonl\"):\n",
        "    print(f\"   üìä Dataset found: train_dataset.jsonl ({os.path.getsize('train_dataset.jsonl')/1024:.1f} KB)\")\n",
        "else:\n",
        "    print(\"   ‚ö†Ô∏è Dataset not found - will need to upload train_dataset.jsonl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìÅ Step 4: Dataset Check\n",
        "\n",
        "Check for existing dataset or upload your `train_dataset.jsonl` file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for existing dataset or upload\n",
        "import os\n",
        "\n",
        "dataset_file = \"train_dataset.jsonl\"\n",
        "\n",
        "# Check if dataset already exists\n",
        "if os.path.exists(dataset_file):\n",
        "    print(f\"‚úÖ Found existing dataset: {dataset_file}\")\n",
        "    print(f\"üìä File size: {os.path.getsize(dataset_file) / 1024:.1f} KB\")\n",
        "else:\n",
        "    print(\"üìÅ Dataset not found locally. Please upload your train_dataset.jsonl file:\")\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        uploaded = files.upload()\n",
        "        \n",
        "        if dataset_file in uploaded:\n",
        "            print(f\"‚úÖ Dataset uploaded successfully: {dataset_file}\")\n",
        "            print(f\"üìä File size: {os.path.getsize(dataset_file) / 1024:.1f} KB\")\n",
        "        else:\n",
        "            print(\"‚ùå Dataset file not found. Please upload train_dataset.jsonl\")\n",
        "            raise FileNotFoundError(\"Dataset file is required to proceed\")\n",
        "    except ImportError:\n",
        "        # Not in Colab environment\n",
        "        print(\"‚ÑπÔ∏è Not in Colab environment. Please ensure train_dataset.jsonl is in the current directory.\")\n",
        "        if not os.path.exists(dataset_file):\n",
        "            raise FileNotFoundError(f\"Dataset file '{dataset_file}' not found in current directory\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚öôÔ∏è Step 5: Model Configuration\n",
        "\n",
        "Configure the base model, quantization settings, and training parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
        "from peft import LoraConfig\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# Model and training configuration\n",
        "model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "new_model_name = \"phi-3-mini-kantra-rules-generator\"\n",
        "\n",
        "print(f\"ü§ñ Base model: {model_id}\")\n",
        "print(f\"üéØ Output model: {new_model_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure quantization for memory efficiency (QLoRA)\n",
        "bnb_config = None\n",
        "if use_quantization:\n",
        "    try:\n",
        "        # BitsAndBytesConfig should already be imported above, but let's be safe\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        )\n",
        "        print(\"‚ö° 4-bit quantization enabled for memory efficiency\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Quantization setup failed: {e}\")\n",
        "        print(\"üîÑ Disabling quantization and continuing...\")\n",
        "        use_quantization = False\n",
        "        bnb_config = None\n",
        "        \n",
        "if not use_quantization:\n",
        "    print(\"‚ö†Ô∏è Running without quantization (will use more memory)\")\n",
        "    print(\"üí° This is normal if bitsandbytes installation failed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì• Step 6: Load Model and Tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load tokenizer and model\n",
        "print(\"üìù Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "print(\"ü§ñ Loading Phi-3 model...\")\n",
        "model_kwargs = {\n",
        "    \"trust_remote_code\": True,\n",
        "    \"attn_implementation\": \"flash_attention_2\" if use_quantization else \"eager\",\n",
        "}\n",
        "\n",
        "if bnb_config is not None:\n",
        "    model_kwargs[\"quantization_config\"] = bnb_config\n",
        "    model_kwargs[\"device_map\"] = \"auto\"\n",
        "    model_kwargs[\"dtype\"] = torch.bfloat16\n",
        "else:\n",
        "    model_kwargs[\"dtype\"] = torch.float32\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, **model_kwargs)\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "\n",
        "print(\"‚úÖ Model and tokenizer loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéõÔ∏è Step 7: Configure LoRA and Load Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure LoRA for parameter-efficient fine-tuning\n",
        "lora_config = LoraConfig(\n",
        "    r=16,                # Rank of the update matrices\n",
        "    lora_alpha=32,       # Alpha parameter for scaling\n",
        "    lora_dropout=0.05,   # Dropout probability for LoRA layers\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        ")\n",
        "\n",
        "# Load the dataset\n",
        "print(\"üìä Loading training dataset...\")\n",
        "dataset = load_dataset(\"json\", data_files=dataset_file, split=\"train\")\n",
        "\n",
        "print(f\"‚úÖ Dataset loaded: {len(dataset):,} examples\")\n",
        "print(f\"üéØ LoRA will train only ~1% of model parameters!\")\n",
        "\n",
        "# Show a sample\n",
        "print(\"\\nüìù Sample training example:\")\n",
        "print(\"=\" * 50)\n",
        "sample = dataset[0]\n",
        "for key, value in sample.items():\n",
        "    if isinstance(value, str) and len(value) > 200:\n",
        "        print(f\"{key}: {value[:200]}...\")\n",
        "    else:\n",
        "        print(f\"{key}: {value}\")\n",
        "print(\"=\" * 50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üèãÔ∏è Step 8: Training Configuration and Start Training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Fix Training Arguments (Run this if you get ValueError)\n",
        "\n",
        "If you get a ValueError about `load_best_model_at_end`, run this cell to fix the training arguments.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fixed training arguments (without load_best_model_at_end)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=f\"./{new_model_name}\",\n",
        "    per_device_train_batch_size=2 if use_quantization else 1,\n",
        "    gradient_accumulation_steps=2 if use_quantization else 4,\n",
        "    learning_rate=2e-4,\n",
        "    logging_steps=10,\n",
        "    num_train_epochs=3,\n",
        "    save_strategy=\"epoch\",\n",
        "    optim=\"paged_adamw_32bit\" if use_quantization else \"adamw_torch\",\n",
        "    fp16=False,\n",
        "    bf16=use_quantization,\n",
        "    dataloader_pin_memory=True,\n",
        "    remove_unused_columns=False,\n",
        "    report_to=None,  # Disable wandb\n",
        "    save_total_limit=2,  # Keep only the last 2 checkpoints to save space\n",
        "    # Removed load_best_model_at_end since we don't have validation data\n",
        "    # This parameter requires eval_strategy to be set, which we don't need for this training\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Training arguments fixed!\")\n",
        "print(\"üí° Removed load_best_model_at_end to avoid validation requirement\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Fix Flash Attention Error (Run this if training fails)\n",
        "\n",
        "If you get a \"FlashAttention only supports Ampere GPUs or newer\" error, run this cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fix Flash Attention and Wandb issues\n",
        "import os\n",
        "\n",
        "# Disable wandb completely\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
        "\n",
        "# Recreate training arguments with eager attention and disabled wandb\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=f\"./{new_model_name}\",\n",
        "    per_device_train_batch_size=2 if use_quantization else 1,\n",
        "    gradient_accumulation_steps=2 if use_quantization else 4,\n",
        "    learning_rate=2e-4,\n",
        "    logging_steps=10,\n",
        "    num_train_epochs=3,\n",
        "    save_strategy=\"epoch\",\n",
        "    optim=\"paged_adamw_32bit\" if use_quantization else \"adamw_torch\",\n",
        "    fp16=False,\n",
        "    bf16=use_quantization,\n",
        "    dataloader_pin_memory=True,\n",
        "    remove_unused_columns=False,\n",
        "    report_to=[],  # Empty list to disable all reporting\n",
        "    save_total_limit=2,\n",
        ")\n",
        "\n",
        "# Reload model with eager attention (no flash attention)\n",
        "print(\"üîÑ Reloading model with eager attention...\")\n",
        "\n",
        "# Clear memory first\n",
        "if 'model' in locals():\n",
        "    del model\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Reload model with eager attention\n",
        "model_kwargs = {\n",
        "    \"trust_remote_code\": True,\n",
        "    \"attn_implementation\": \"eager\",  # Force eager attention for T4 compatibility\n",
        "}\n",
        "\n",
        "if bnb_config is not None:\n",
        "    model_kwargs[\"quantization_config\"] = bnb_config\n",
        "    model_kwargs[\"device_map\"] = \"auto\"\n",
        "    model_kwargs[\"dtype\"] = torch.bfloat16\n",
        "else:\n",
        "    model_kwargs[\"dtype\"] = torch.float32\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, **model_kwargs)\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "\n",
        "print(\"‚úÖ Model reloaded with eager attention\")\n",
        "print(\"‚úÖ Wandb disabled\")\n",
        "print(\"üí° Ready to train on T4 GPU!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Recreate trainer with fixed configuration\n",
        "print(\"üöÄ Creating trainer with T4-compatible settings...\")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    args=training_args,\n",
        "    peft_config=lora_config,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Trainer created successfully!\")\n",
        "print(\"üéØ Configuration:\")\n",
        "print(f\"   - Attention: eager (T4 compatible)\")\n",
        "print(f\"   - Wandb: disabled\")\n",
        "print(f\"   - Quantization: {use_quantization}\")\n",
        "print(f\"   - Device: {model.device}\")\n",
        "\n",
        "# Show memory usage\n",
        "if torch.cuda.is_available():\n",
        "    memory_used = torch.cuda.memory_allocated() / 1e9\n",
        "    memory_total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"   - GPU Memory: {memory_used:.1f}GB / {memory_total:.1f}GB ({memory_used/memory_total*100:.1f}%)\")\n",
        "\n",
        "print(\"\\nüèÉ Ready to start training!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=f\"./{new_model_name}\",\n",
        "    per_device_train_batch_size=2 if use_quantization else 1,\n",
        "    gradient_accumulation_steps=2 if use_quantization else 4,\n",
        "    learning_rate=2e-4,\n",
        "    logging_steps=10,\n",
        "    num_train_epochs=3,\n",
        "    save_strategy=\"epoch\",\n",
        "    optim=\"paged_adamw_32bit\" if use_quantization else \"adamw_torch\",\n",
        "    fp16=False,\n",
        "    bf16=use_quantization,\n",
        "    dataloader_pin_memory=True,\n",
        "    remove_unused_columns=False,\n",
        "    report_to=None,  # Disable wandb\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "# Calculate estimates\n",
        "total_examples = len(dataset)\n",
        "batch_size = training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps\n",
        "steps_per_epoch = total_examples // batch_size\n",
        "estimated_time = \"15-30 minutes (GPU)\" if use_quantization else \"2.5-4.5 hours (CPU)\"\n",
        "\n",
        "print(\"üèãÔ∏è Training Configuration:\")\n",
        "print(f\"   üìä Examples: {total_examples:,}\")\n",
        "print(f\"   üî¢ Effective batch size: {batch_size}\")\n",
        "print(f\"   üìà Steps per epoch: {steps_per_epoch:,}\")\n",
        "print(f\"   üîÑ Epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"   ‚è±Ô∏è Estimated time: {estimated_time}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create trainer and start training\n",
        "import time\n",
        "\n",
        "print(\"üöÄ Initializing trainer...\")\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    args=training_args,\n",
        "    peft_config=lora_config,\n",
        ")\n",
        "\n",
        "print(\"üèÉ Starting fine-tuning process...\")\n",
        "print(f\"‚è∞ Started at: {time.strftime('%H:%M:%S')}\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üî• TRAINING IN PROGRESS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "start_time = time.time()\n",
        "trainer.train()\n",
        "end_time = time.time()\n",
        "\n",
        "training_time = end_time - start_time\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üéâ TRAINING COMPLETED!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"‚è±Ô∏è Total training time: {training_time/60:.1f} minutes\")\n",
        "print(f\"‚è∞ Finished at: {time.strftime('%H:%M:%S')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üíæ Step 9: Save Model and Test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the fine-tuned model\n",
        "final_model_path = f\"./{new_model_name}-final\"\n",
        "print(f\"üíæ Saving model to: {final_model_path}\")\n",
        "trainer.save_model(final_model_path)\n",
        "print(\"‚úÖ Model saved successfully!\")\n",
        "\n",
        "# Create downloadable archive (for Colab)\n",
        "try:\n",
        "    import shutil\n",
        "    archive_name = f\"{new_model_name}-final\"\n",
        "    shutil.make_archive(archive_name, 'zip', final_model_path)\n",
        "    archive_size = os.path.getsize(f\"{archive_name}.zip\") / 1024 / 1024\n",
        "    print(f\"üì¶ Archive created: {archive_name}.zip ({archive_size:.1f} MB)\")\n",
        "    print(\"üì• Download from Colab file browser!\")\n",
        "except:\n",
        "    print(\"‚ÑπÔ∏è Archive creation skipped (not in Colab or error occurred)\")\n",
        "\n",
        "print(f\"\\nüéä Fine-tuning complete! Your model is ready to use.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the fine-tuned model\n",
        "print(\"üß™ Testing the fine-tuned model...\")\n",
        "\n",
        "test_prompt = \"Generate a Kantra rule to detect when a Java file imports `sun.misc.Unsafe`, which is a non-portable and risky API.\"\n",
        "\n",
        "messages = [{\"role\": \"user\", \"content\": test_prompt}]\n",
        "model_inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "print(f\"üéØ Test prompt: {test_prompt}\")\n",
        "print(\"\\nü§ñ Generating response...\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    generated_ids = model.generate(\n",
        "        model_inputs,\n",
        "        max_new_tokens=500,\n",
        "        do_sample=True,\n",
        "        temperature=0.1,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "decoded_output = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "if \"<|assistant|>\" in decoded_output:\n",
        "    response = decoded_output.split(\"<|assistant|>\")[1].strip()\n",
        "else:\n",
        "    input_text = tokenizer.batch_decode(model_inputs, skip_special_tokens=True)[0]\n",
        "    response = decoded_output.replace(input_text, \"\").strip()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üéâ MODEL RESPONSE\")\n",
        "print(\"=\"*60)\n",
        "print(response)\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Validate YAML\n",
        "try:\n",
        "    import yaml\n",
        "    yaml.safe_load(response)\n",
        "    print(\"‚úÖ Success! The output appears to be valid YAML.\")\n",
        "except:\n",
        "    print(\"‚ÑπÔ∏è YAML validation skipped or output may not be valid YAML\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîÑ Step 10: Merge LoRA Adapter (Optional)\n",
        "\n",
        "**‚ö†Ô∏è Memory Requirements:**\n",
        "- **Phi-3-mini (3.8B)**: ~8GB RAM needed for merging\n",
        "- **Colab Free**: 12GB RAM available ‚úÖ **Should work**\n",
        "- **Colab Pro**: 25GB+ RAM available ‚úÖ **Will work**\n",
        "\n",
        "**Note**: Larger models (7B+) need 18GB+ RAM and won't work on Colab Free.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check available memory and decide whether to merge\n",
        "import psutil\n",
        "import gc\n",
        "\n",
        "def get_available_memory_gb():\n",
        "    \"\"\"Get available RAM in GB\"\"\"\n",
        "    return psutil.virtual_memory().available / (1024**3)\n",
        "\n",
        "def get_total_memory_gb():\n",
        "    \"\"\"Get total RAM in GB\"\"\"\n",
        "    return psutil.virtual_memory().total / (1024**3)\n",
        "\n",
        "available_memory = get_available_memory_gb()\n",
        "total_memory = get_total_memory_gb()\n",
        "\n",
        "print(f\"üíæ System Memory:\")\n",
        "print(f\"   Total RAM: {total_memory:.1f} GB\")\n",
        "print(f\"   Available RAM: {available_memory:.1f} GB\")\n",
        "\n",
        "# Memory requirements for different models\n",
        "memory_requirements = {\n",
        "    \"phi-3-mini\": 8,  # 3.8B parameters\n",
        "    \"phi-3-small\": 12, # 7B parameters  \n",
        "    \"phi-3-medium\": 18, # 14B parameters\n",
        "}\n",
        "\n",
        "model_size = \"phi-3-mini\"  # We're using Phi-3-mini\n",
        "required_memory = memory_requirements[model_size]\n",
        "\n",
        "print(f\"üìä Model: {model_size}\")\n",
        "print(f\"üîß Required RAM for merging: ~{required_memory} GB\")\n",
        "\n",
        "can_merge = available_memory >= required_memory\n",
        "print(f\"‚úÖ Can merge LoRA: {can_merge}\")\n",
        "\n",
        "if not can_merge:\n",
        "    print(f\"‚ö†Ô∏è Warning: Only {available_memory:.1f}GB available, need {required_memory}GB\")\n",
        "    print(\"üí° Options:\")\n",
        "    print(\"   1. Skip merging (use LoRA adapter as-is)\")\n",
        "    print(\"   2. Upgrade to Colab Pro for more RAM\")\n",
        "    print(\"   3. Use a local machine with more RAM\")\n",
        "else:\n",
        "    print(\"üéâ Sufficient memory available for merging!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Merge LoRA adapter (if sufficient memory)\n",
        "merge_lora = input(\"ü§î Do you want to merge the LoRA adapter? (y/n): \").lower().strip() == 'y'\n",
        "\n",
        "if merge_lora and can_merge:\n",
        "    print(\"üîÑ Starting LoRA merge process...\")\n",
        "    print(\"‚ö†Ô∏è This may take several minutes and use significant memory...\")\n",
        "    \n",
        "    try:\n",
        "        # Clear memory first\n",
        "        if 'trainer' in locals():\n",
        "            del trainer\n",
        "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "        gc.collect()\n",
        "        \n",
        "        print(\"üì• Loading base model for merging...\")\n",
        "        # Load base model in float16 to save memory\n",
        "        base_model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        "            trust_remote_code=True,\n",
        "        )\n",
        "        \n",
        "        print(\"üîó Loading and merging LoRA adapter...\")\n",
        "        from peft import PeftModel\n",
        "        \n",
        "        # Load the LoRA model\n",
        "        lora_model = PeftModel.from_pretrained(base_model, final_model_path)\n",
        "        \n",
        "        # Merge the adapter\n",
        "        merged_model = lora_model.merge_and_unload()\n",
        "        \n",
        "        # Save the merged model\n",
        "        merged_model_path = f\"./{new_model_name}-merged\"\n",
        "        print(f\"üíæ Saving merged model to: {merged_model_path}\")\n",
        "        \n",
        "        merged_model.save_pretrained(merged_model_path)\n",
        "        tokenizer.save_pretrained(merged_model_path)\n",
        "        \n",
        "        print(\"‚úÖ LoRA merge completed successfully!\")\n",
        "        print(f\"üìÅ Merged model saved to: {merged_model_path}\")\n",
        "        \n",
        "        # Create archive for merged model\n",
        "        try:\n",
        "            import shutil\n",
        "            merged_archive_name = f\"{new_model_name}-merged\"\n",
        "            shutil.make_archive(merged_archive_name, 'zip', merged_model_path)\n",
        "            archive_size = os.path.getsize(f\"{merged_archive_name}.zip\") / 1024 / 1024\n",
        "            print(f\"üì¶ Merged model archive: {merged_archive_name}.zip ({archive_size:.1f} MB)\")\n",
        "        except:\n",
        "            print(\"‚ÑπÔ∏è Could not create merged model archive\")\n",
        "        \n",
        "        # Clean up memory\n",
        "        del base_model, lora_model, merged_model\n",
        "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "        gc.collect()\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error during LoRA merge: {e}\")\n",
        "        print(\"üí° The LoRA adapter is still available for use without merging\")\n",
        "        \n",
        "elif merge_lora and not can_merge:\n",
        "    print(\"‚ùå Cannot merge: Insufficient memory\")\n",
        "    print(f\"üí° Need {required_memory}GB RAM, but only {available_memory:.1f}GB available\")\n",
        "    print(\"üîß The LoRA adapter works fine without merging!\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è Skipping LoRA merge - using adapter format\")\n",
        "    print(\"üí° You can still use the model with the LoRA adapter!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Step 11: Upload to Hugging Face Hub (Optional)\n",
        "\n",
        "Upload your fine-tuned model to Hugging Face Hub for easy sharing and deployment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Upload to Hugging Face Hub\n",
        "upload_to_hub = input(\"ü§î Do you want to upload the model to Hugging Face Hub? (y/n): \").lower().strip() == 'y'\n",
        "\n",
        "if upload_to_hub:\n",
        "    try:\n",
        "        from huggingface_hub import HfApi, login\n",
        "        \n",
        "        print(\"üîë Please log in to Hugging Face Hub...\")\n",
        "        print(\"üí° You'll need a Hugging Face account and access token\")\n",
        "        print(\"üìù Get your token from: https://huggingface.co/settings/tokens\")\n",
        "        \n",
        "        # Login to Hugging Face\n",
        "        login()\n",
        "        \n",
        "        # Get repository name\n",
        "        repo_name = input(\"üìù Enter repository name (e.g., 'your-username/phi3-kantra-rules'): \").strip()\n",
        "        \n",
        "        if not repo_name:\n",
        "            repo_name = f\"phi3-kantra-rules-{int(time.time())}\"\n",
        "            print(f\"üè∑Ô∏è Using default name: {repo_name}\")\n",
        "        \n",
        "        # Determine which model to upload\n",
        "        if 'merged_model_path' in locals() and os.path.exists(merged_model_path):\n",
        "            upload_path = merged_model_path\n",
        "            model_type = \"merged\"\n",
        "            print(f\"üì§ Uploading merged model from: {upload_path}\")\n",
        "        else:\n",
        "            upload_path = final_model_path\n",
        "            model_type = \"LoRA adapter\"\n",
        "            print(f\"üì§ Uploading LoRA adapter from: {upload_path}\")\n",
        "        \n",
        "        # Create repository and upload\n",
        "        api = HfApi()\n",
        "        \n",
        "        print(f\"üèóÔ∏è Creating repository: {repo_name}\")\n",
        "        api.create_repo(repo_id=repo_name, exist_ok=True)\n",
        "        \n",
        "        print(f\"üì§ Uploading {model_type}...\")\n",
        "        api.upload_folder(\n",
        "            folder_path=upload_path,\n",
        "            repo_id=repo_name,\n",
        "            commit_message=f\"Upload fine-tuned Phi-3 mini for Kantra rules generation ({model_type})\"\n",
        "        )\n",
        "        \n",
        "        print(\"‚úÖ Upload completed successfully!\")\n",
        "        print(f\"üîó Your model is available at: https://huggingface.co/{repo_name}\")\n",
        "        print(f\"üí° You can now use it with: AutoModelForCausalLM.from_pretrained('{repo_name}')\\\")\\n\")\n",
        "        \n",
        "        # Create model card\n",
        "        model_card_content = f\\\"\\\"\\\"---\\nlicense: mit\\nbase_model: {model_id}\\ntags:\\n- phi3\\n- kantra\\n- code-migration\\n- fine-tuned\\nlibrary_name: transformers\\n---\\n\\n# Phi-3 Mini Fine-tuned for Kantra Rules Generation\\n\\nThis model is a fine-tuned version of [{model_id}](https://huggingface.co/{model_id}) for generating Kantra migration rules.\\n\\n## Model Details\\n- **Base Model**: {model_id}\\n- **Fine-tuning Method**: LoRA (Low-Rank Adaptation)\\n- **Task**: Code migration rule generation\\n- **Model Type**: {model_type}\\n\\n## Usage\\n\\n```python\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\n{'from peft import PeftModel' if model_type == 'LoRA adapter' else ''}\\n\\ntokenizer = AutoTokenizer.from_pretrained(\\\"{repo_name}\\\")\\n{'base_model = AutoModelForCausalLM.from_pretrained(\\\"' + model_id + '\\\")' if model_type == 'LoRA adapter' else ''}\\n{'model = PeftModel.from_pretrained(base_model, \\\"' + repo_name + '\\\")' if model_type == 'LoRA adapter' else 'model = AutoModelForCausalLM.from_pretrained(\\\"' + repo_name + '\\\")'}\\n\\n# Generate Kantra rules\\nprompt = \\\"Generate a Kantra rule to detect deprecated Java APIs\\\"\\nmessages = [{{\\\"role\\\": \\\"user\\\", \\\"content\\\": prompt}}]\\ninputs = tokenizer.apply_chat_template(messages, return_tensors=\\\"pt\\\")\\noutputs = model.generate(inputs, max_new_tokens=500)\\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\\nprint(response)\\n```\\n\\n## Training Details\\n- Fine-tuned using QLoRA for parameter efficiency\\n- Optimized for generating YAML-formatted Kantra migration rules\\n- Trained on custom Kantra rules dataset\\n\\\"\\\"\\\"\\n        \\n        # Upload model card\\n        with open(\\\"README.md\\\", \\\"w\\\") as f:\\n            f.write(model_card_content)\\n        \\n        api.upload_file(\\n            path_or_fileobj=\\\"README.md\\\",\\n            path_in_repo=\\\"README.md\\\",\\n            repo_id=repo_name,\\n            commit_message=\\\"Add model card\\\"\\n        )\\n        \\n        os.remove(\\\"README.md\\\")  # Clean up\\n        \\n        print(\\\"üìÑ Model card created and uploaded!\\\")\\n        \\n    except ImportError:\\n        print(\\\"‚ùå huggingface_hub not installed. Install with: pip install huggingface_hub\\\")\\n    except Exception as e:\\n        print(f\\\"‚ùå Upload failed: {e}\\\")\\n        print(\\\"üí° You can manually upload the model files to Hugging Face Hub\\\")\\nelse:\\n    print(\\\"‚ÑπÔ∏è Skipping Hugging Face Hub upload\\\")\\n    print(\\\"üí° You can manually upload later if needed\\\")\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéä Summary & Next Steps\n",
        "\n",
        "Congratulations! You've successfully fine-tuned Phi-3-mini for Kantra rules generation.\n",
        "\n",
        "### ‚úÖ What you accomplished:\n",
        "- ‚úÖ Fine-tuned a 3.8B parameter model using LoRA\n",
        "- ‚úÖ Used parameter-efficient training (~1% of parameters)\n",
        "- ‚úÖ Created a specialized model for migration rule generation\n",
        "- ‚úÖ Handled memory constraints intelligently\n",
        "- ‚úÖ Generated downloadable model files\n",
        "\n",
        "### üìÅ Your files:\n",
        "- **LoRA Adapter**: `phi-3-mini-kantra-rules-generator-final/` (always created)\n",
        "- **Merged Model**: `phi-3-mini-kantra-rules-generator-merged/` (if merged)\n",
        "- **Archives**: `.zip` files for easy download\n",
        "\n",
        "### üîÑ Model Formats Explained:\n",
        "\n",
        "#### **LoRA Adapter** (Always Available):\n",
        "- ‚úÖ **Small size**: Only adapter weights (~few MB)\n",
        "- ‚úÖ **Memory efficient**: Works on any system\n",
        "- ‚úÖ **Flexible**: Can be applied to different base models\n",
        "- ‚ö†Ô∏è **Requires base model**: Need to load Phi-3-mini + adapter\n",
        "\n",
        "#### **Merged Model** (If you have enough RAM):\n",
        "- ‚úÖ **Standalone**: Complete model, no base model needed\n",
        "- ‚úÖ **Faster loading**: Single model file\n",
        "- ‚úÖ **Easy deployment**: Standard transformer model\n",
        "- ‚ö†Ô∏è **Larger size**: Full model weights (~7GB)\n",
        "\n",
        "### üöÄ Next steps:\n",
        "\n",
        "#### **1. Local Testing**:\n",
        "```python\n",
        "# For LoRA adapter:\n",
        "from peft import PeftModel\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
        "model = PeftModel.from_pretrained(base_model, \"./phi-3-mini-kantra-rules-generator-final\")\n",
        "\n",
        "# For merged model (if available):\n",
        "model = AutoModelForCausalLM.from_pretrained(\"./phi-3-mini-kantra-rules-generator-merged\")\n",
        "```\n",
        "\n",
        "#### **2. Production Deployment**:\n",
        "- Use merged model for faster inference\n",
        "- Use LoRA adapter for memory-constrained environments\n",
        "- Consider uploading to Hugging Face Hub for easy access\n",
        "\n",
        "#### **3. Further Improvements**:\n",
        "- üìä Add validation dataset to monitor overfitting\n",
        "- üîß Experiment with different LoRA ranks (8, 32, 64)\n",
        "- üìà Try different learning rates (1e-4, 5e-4)\n",
        "- üìù Add more diverse training examples\n",
        "\n",
        "### üí° Memory Guidelines:\n",
        "- **Colab Free (12GB)**: ‚úÖ Phi-3-mini merging works\n",
        "- **Colab Pro (25GB+)**: ‚úÖ All models work\n",
        "- **Local Mac**: Depends on unified memory\n",
        "- **For 7B+ models**: Need 18GB+ RAM for merging\n",
        "\n",
        "---\n",
        "**üéâ Happy fine-tuning! Your Kantra rules generator is ready to use! üöÄ**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ ALTERNATIVE: Run Robust Training Pipeline\n",
        "\n",
        "## üéØ **Use This Instead of Individual Steps Above**\n",
        "\n",
        "If you want the most robust training with automatic model merging, run this cell instead of the individual steps above. This implements all the improvements in one go.\n",
        "\n",
        "**Key Features:**\n",
        "- ‚úÖ Explicit chat template formatting\n",
        "- ‚úÖ System prompt integration  \n",
        "- ‚úÖ Automatic LoRA merging\n",
        "- ‚úÖ Standalone model creation\n",
        "- ‚úÖ Built-in testing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the robust training pipeline\n",
        "!python robust_finetune.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üß™ Test the Robust Model\n",
        "\n",
        "## üéØ **Simple Inference with Merged Model**\n",
        "\n",
        "The robust pipeline creates a merged model that's much simpler to use. No PEFT/LoRA complexity!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the merged model with simple inference\n",
        "!python simple_inference.py\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
