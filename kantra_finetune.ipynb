{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸš€ Phi-3 Mini Fine-tuning for Kantra Rules Generation\n",
        "\n",
        "This notebook fine-tunes Microsoft's Phi-3-mini-4k-instruct model to generate Kantra migration rules using LoRA (Low-Rank Adaptation) for parameter-efficient training.\n",
        "\n",
        "## ğŸ“‹ What this notebook does:\n",
        "- Fine-tunes Phi-3-mini (3.8B params) using QLoRA\n",
        "- Uses your custom Kantra rules dataset\n",
        "- Optimized for Google Colab (free GPU)\n",
        "- Creates a specialized model for migration rule generation\n",
        "\n",
        "## âš¡ Expected Performance:\n",
        "- **Colab Free (T4)**: ~15-30 minutes training time\n",
        "- **Colab Pro (A100)**: ~5-15 minutes training time\n",
        "- **Local Mac**: ~2.5-4.5 hours (not recommended)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## âš ï¸ Step 1: GPU Check\n",
        "\n",
        "**IMPORTANT**: Make sure you're using a GPU runtime before proceeding!\n",
        "\n",
        "Go to: **Runtime** â†’ **Change runtime type** â†’ **Hardware accelerator** â†’ **T4 GPU**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify GPU is available\n",
        "import torch\n",
        "\n",
        "try:\n",
        "    assert torch.cuda.is_available() is True\n",
        "    print(\"âœ… GPU is available!\")\n",
        "    print(f\"ğŸš€ Using GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"ğŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "except AssertionError:\n",
        "    print(\"âŒ GPU is not available!\")\n",
        "    print(\"âš ï¸ Please set up a GPU before using this notebook:\")\n",
        "    print(\"   1. Go to Runtime â†’ Change runtime type\")\n",
        "    print(\"   2. Select 'T4 GPU' under Hardware accelerator\")\n",
        "    print(\"   3. Click Save and restart the runtime\")\n",
        "    print(\"   4. Re-run this cell\")\n",
        "    raise RuntimeError(\"GPU required for efficient fine-tuning. Please enable GPU and restart.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“¥ Step 2: Clone Repository\n",
        "\n",
        "Clone the Kantra fine-tuning repository and install dependencies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone the repository and set up environment\n",
        "%cd /content/\n",
        "%rm -rf kantra-finetune\n",
        "!git clone --depth 1 https://github.com/sshaaf/kantra-finetune.git\n",
        "%cd kantra-finetune\n",
        "%ls\n",
        "\n",
        "# Install dependencies from the repository\n",
        "!pip install -e .[torch,bitsandbytes]\n",
        "\n",
        "# Install additional packages for fine-tuning\n",
        "!pip install transformers datasets accelerate peft trl\n",
        "!pip install flash-attn --no-build-isolation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ”§ Step 3: Environment Setup\n",
        "\n",
        "Verify the installation and check hardware configuration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify installation and setup\n",
        "import torch\n",
        "import sys\n",
        "import os\n",
        "\n",
        "print(f\"ğŸ“ Current directory: {os.getcwd()}\")\n",
        "print(f\"ğŸ Python version: {sys.version}\")\n",
        "print(f\"ğŸ”¥ PyTorch version: {torch.__version__}\")\n",
        "print(f\"âš¡ CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"ğŸš€ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"ğŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"âŒ No GPU available - this will be very slow!\")\n",
        "\n",
        "# Set device configuration\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "use_quantization = torch.cuda.is_available()\n",
        "\n",
        "print(f\"\\nâœ… Configuration:\")\n",
        "print(f\"   ğŸ¯ Device: {device}\")\n",
        "print(f\"   ğŸ“¦ Quantization: {use_quantization}\")\n",
        "print(f\"   ğŸ“ Working directory: {os.getcwd()}\")\n",
        "\n",
        "# Verify key files exist\n",
        "if os.path.exists(\"train_dataset.jsonl\"):\n",
        "    print(f\"   ğŸ“Š Dataset found: train_dataset.jsonl ({os.path.getsize('train_dataset.jsonl')/1024:.1f} KB)\")\n",
        "else:\n",
        "    print(\"   âš ï¸ Dataset not found - will need to upload train_dataset.jsonl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“ Step 4: Dataset Check\n",
        "\n",
        "Check for existing dataset or upload your `train_dataset.jsonl` file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for existing dataset or upload\n",
        "import os\n",
        "\n",
        "dataset_file = \"train_dataset.jsonl\"\n",
        "\n",
        "# Check if dataset already exists\n",
        "if os.path.exists(dataset_file):\n",
        "    print(f\"âœ… Found existing dataset: {dataset_file}\")\n",
        "    print(f\"ğŸ“Š File size: {os.path.getsize(dataset_file) / 1024:.1f} KB\")\n",
        "else:\n",
        "    print(\"ğŸ“ Dataset not found locally. Please upload your train_dataset.jsonl file:\")\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        uploaded = files.upload()\n",
        "        \n",
        "        if dataset_file in uploaded:\n",
        "            print(f\"âœ… Dataset uploaded successfully: {dataset_file}\")\n",
        "            print(f\"ğŸ“Š File size: {os.path.getsize(dataset_file) / 1024:.1f} KB\")\n",
        "        else:\n",
        "            print(\"âŒ Dataset file not found. Please upload train_dataset.jsonl\")\n",
        "            raise FileNotFoundError(\"Dataset file is required to proceed\")\n",
        "    except ImportError:\n",
        "        # Not in Colab environment\n",
        "        print(\"â„¹ï¸ Not in Colab environment. Please ensure train_dataset.jsonl is in the current directory.\")\n",
        "        if not os.path.exists(dataset_file):\n",
        "            raise FileNotFoundError(f\"Dataset file '{dataset_file}' not found in current directory\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## âš™ï¸ Step 5: Model Configuration\n",
        "\n",
        "Configure the base model, quantization settings, and training parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
        "from peft import LoraConfig\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# Model and training configuration\n",
        "model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "new_model_name = \"phi-3-mini-kantra-rules-generator\"\n",
        "\n",
        "print(f\"ğŸ¤– Base model: {model_id}\")\n",
        "print(f\"ğŸ¯ Output model: {new_model_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure quantization for memory efficiency (QLoRA)\n",
        "bnb_config = None\n",
        "if use_quantization:\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    )\n",
        "    print(\"âš¡ 4-bit quantization enabled for memory efficiency\")\n",
        "else:\n",
        "    print(\"âš ï¸ Running without quantization (will use more memory)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“¥ Step 6: Load Model and Tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load tokenizer and model\n",
        "print(\"ğŸ“ Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "print(\"ğŸ¤– Loading Phi-3 model...\")\n",
        "model_kwargs = {\n",
        "    \"trust_remote_code\": True,\n",
        "    \"attn_implementation\": \"flash_attention_2\" if use_quantization else \"eager\",\n",
        "}\n",
        "\n",
        "if bnb_config is not None:\n",
        "    model_kwargs[\"quantization_config\"] = bnb_config\n",
        "    model_kwargs[\"device_map\"] = \"auto\"\n",
        "    model_kwargs[\"dtype\"] = torch.bfloat16\n",
        "else:\n",
        "    model_kwargs[\"dtype\"] = torch.float32\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, **model_kwargs)\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "\n",
        "print(\"âœ… Model and tokenizer loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ›ï¸ Step 7: Configure LoRA and Load Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure LoRA for parameter-efficient fine-tuning\n",
        "lora_config = LoraConfig(\n",
        "    r=16,                # Rank of the update matrices\n",
        "    lora_alpha=32,       # Alpha parameter for scaling\n",
        "    lora_dropout=0.05,   # Dropout probability for LoRA layers\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        ")\n",
        "\n",
        "# Load the dataset\n",
        "print(\"ğŸ“Š Loading training dataset...\")\n",
        "dataset = load_dataset(\"json\", data_files=dataset_file, split=\"train\")\n",
        "\n",
        "print(f\"âœ… Dataset loaded: {len(dataset):,} examples\")\n",
        "print(f\"ğŸ¯ LoRA will train only ~1% of model parameters!\")\n",
        "\n",
        "# Show a sample\n",
        "print(\"\\nğŸ“ Sample training example:\")\n",
        "print(\"=\" * 50)\n",
        "sample = dataset[0]\n",
        "for key, value in sample.items():\n",
        "    if isinstance(value, str) and len(value) > 200:\n",
        "        print(f\"{key}: {value[:200]}...\")\n",
        "    else:\n",
        "        print(f\"{key}: {value}\")\n",
        "print(\"=\" * 50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ‹ï¸ Step 8: Training Configuration and Start Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=f\"./{new_model_name}\",\n",
        "    per_device_train_batch_size=2 if use_quantization else 1,\n",
        "    gradient_accumulation_steps=2 if use_quantization else 4,\n",
        "    learning_rate=2e-4,\n",
        "    logging_steps=10,\n",
        "    num_train_epochs=3,\n",
        "    save_strategy=\"epoch\",\n",
        "    optim=\"paged_adamw_32bit\" if use_quantization else \"adamw_torch\",\n",
        "    fp16=False,\n",
        "    bf16=use_quantization,\n",
        "    dataloader_pin_memory=True,\n",
        "    remove_unused_columns=False,\n",
        "    report_to=None,  # Disable wandb\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "# Calculate estimates\n",
        "total_examples = len(dataset)\n",
        "batch_size = training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps\n",
        "steps_per_epoch = total_examples // batch_size\n",
        "estimated_time = \"15-30 minutes (GPU)\" if use_quantization else \"2.5-4.5 hours (CPU)\"\n",
        "\n",
        "print(\"ğŸ‹ï¸ Training Configuration:\")\n",
        "print(f\"   ğŸ“Š Examples: {total_examples:,}\")\n",
        "print(f\"   ğŸ”¢ Effective batch size: {batch_size}\")\n",
        "print(f\"   ğŸ“ˆ Steps per epoch: {steps_per_epoch:,}\")\n",
        "print(f\"   ğŸ”„ Epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"   â±ï¸ Estimated time: {estimated_time}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create trainer and start training\n",
        "import time\n",
        "\n",
        "print(\"ğŸš€ Initializing trainer...\")\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    args=training_args,\n",
        "    peft_config=lora_config,\n",
        ")\n",
        "\n",
        "print(\"ğŸƒ Starting fine-tuning process...\")\n",
        "print(f\"â° Started at: {time.strftime('%H:%M:%S')}\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸ”¥ TRAINING IN PROGRESS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "start_time = time.time()\n",
        "trainer.train()\n",
        "end_time = time.time()\n",
        "\n",
        "training_time = end_time - start_time\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸ‰ TRAINING COMPLETED!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"â±ï¸ Total training time: {training_time/60:.1f} minutes\")\n",
        "print(f\"â° Finished at: {time.strftime('%H:%M:%S')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ’¾ Step 9: Save Model and Test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the fine-tuned model\n",
        "final_model_path = f\"./{new_model_name}-final\"\n",
        "print(f\"ğŸ’¾ Saving model to: {final_model_path}\")\n",
        "trainer.save_model(final_model_path)\n",
        "print(\"âœ… Model saved successfully!\")\n",
        "\n",
        "# Create downloadable archive (for Colab)\n",
        "try:\n",
        "    import shutil\n",
        "    archive_name = f\"{new_model_name}-final\"\n",
        "    shutil.make_archive(archive_name, 'zip', final_model_path)\n",
        "    archive_size = os.path.getsize(f\"{archive_name}.zip\") / 1024 / 1024\n",
        "    print(f\"ğŸ“¦ Archive created: {archive_name}.zip ({archive_size:.1f} MB)\")\n",
        "    print(\"ğŸ“¥ Download from Colab file browser!\")\n",
        "except:\n",
        "    print(\"â„¹ï¸ Archive creation skipped (not in Colab or error occurred)\")\n",
        "\n",
        "print(f\"\\nğŸŠ Fine-tuning complete! Your model is ready to use.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the fine-tuned model\n",
        "print(\"ğŸ§ª Testing the fine-tuned model...\")\n",
        "\n",
        "test_prompt = \"Generate a Kantra rule to detect when a Java file imports `sun.misc.Unsafe`, which is a non-portable and risky API.\"\n",
        "\n",
        "messages = [{\"role\": \"user\", \"content\": test_prompt}]\n",
        "model_inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "print(f\"ğŸ¯ Test prompt: {test_prompt}\")\n",
        "print(\"\\nğŸ¤– Generating response...\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    generated_ids = model.generate(\n",
        "        model_inputs,\n",
        "        max_new_tokens=500,\n",
        "        do_sample=True,\n",
        "        temperature=0.1,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "decoded_output = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "if \"<|assistant|>\" in decoded_output:\n",
        "    response = decoded_output.split(\"<|assistant|>\")[1].strip()\n",
        "else:\n",
        "    input_text = tokenizer.batch_decode(model_inputs, skip_special_tokens=True)[0]\n",
        "    response = decoded_output.replace(input_text, \"\").strip()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸ‰ MODEL RESPONSE\")\n",
        "print(\"=\"*60)\n",
        "print(response)\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Validate YAML\n",
        "try:\n",
        "    import yaml\n",
        "    yaml.safe_load(response)\n",
        "    print(\"âœ… Success! The output appears to be valid YAML.\")\n",
        "except:\n",
        "    print(\"â„¹ï¸ YAML validation skipped or output may not be valid YAML\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ”„ Step 10: Merge LoRA Adapter (Optional)\n",
        "\n",
        "**âš ï¸ Memory Requirements:**\n",
        "- **Phi-3-mini (3.8B)**: ~8GB RAM needed for merging\n",
        "- **Colab Free**: 12GB RAM available âœ… **Should work**\n",
        "- **Colab Pro**: 25GB+ RAM available âœ… **Will work**\n",
        "\n",
        "**Note**: Larger models (7B+) need 18GB+ RAM and won't work on Colab Free.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check available memory and decide whether to merge\n",
        "import psutil\n",
        "import gc\n",
        "\n",
        "def get_available_memory_gb():\n",
        "    \"\"\"Get available RAM in GB\"\"\"\n",
        "    return psutil.virtual_memory().available / (1024**3)\n",
        "\n",
        "def get_total_memory_gb():\n",
        "    \"\"\"Get total RAM in GB\"\"\"\n",
        "    return psutil.virtual_memory().total / (1024**3)\n",
        "\n",
        "available_memory = get_available_memory_gb()\n",
        "total_memory = get_total_memory_gb()\n",
        "\n",
        "print(f\"ğŸ’¾ System Memory:\")\n",
        "print(f\"   Total RAM: {total_memory:.1f} GB\")\n",
        "print(f\"   Available RAM: {available_memory:.1f} GB\")\n",
        "\n",
        "# Memory requirements for different models\n",
        "memory_requirements = {\n",
        "    \"phi-3-mini\": 8,  # 3.8B parameters\n",
        "    \"phi-3-small\": 12, # 7B parameters  \n",
        "    \"phi-3-medium\": 18, # 14B parameters\n",
        "}\n",
        "\n",
        "model_size = \"phi-3-mini\"  # We're using Phi-3-mini\n",
        "required_memory = memory_requirements[model_size]\n",
        "\n",
        "print(f\"ğŸ“Š Model: {model_size}\")\n",
        "print(f\"ğŸ”§ Required RAM for merging: ~{required_memory} GB\")\n",
        "\n",
        "can_merge = available_memory >= required_memory\n",
        "print(f\"âœ… Can merge LoRA: {can_merge}\")\n",
        "\n",
        "if not can_merge:\n",
        "    print(f\"âš ï¸ Warning: Only {available_memory:.1f}GB available, need {required_memory}GB\")\n",
        "    print(\"ğŸ’¡ Options:\")\n",
        "    print(\"   1. Skip merging (use LoRA adapter as-is)\")\n",
        "    print(\"   2. Upgrade to Colab Pro for more RAM\")\n",
        "    print(\"   3. Use a local machine with more RAM\")\n",
        "else:\n",
        "    print(\"ğŸ‰ Sufficient memory available for merging!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Merge LoRA adapter (if sufficient memory)\n",
        "merge_lora = input(\"ğŸ¤” Do you want to merge the LoRA adapter? (y/n): \").lower().strip() == 'y'\n",
        "\n",
        "if merge_lora and can_merge:\n",
        "    print(\"ğŸ”„ Starting LoRA merge process...\")\n",
        "    print(\"âš ï¸ This may take several minutes and use significant memory...\")\n",
        "    \n",
        "    try:\n",
        "        # Clear memory first\n",
        "        if 'trainer' in locals():\n",
        "            del trainer\n",
        "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "        gc.collect()\n",
        "        \n",
        "        print(\"ğŸ“¥ Loading base model for merging...\")\n",
        "        # Load base model in float16 to save memory\n",
        "        base_model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        "            trust_remote_code=True,\n",
        "        )\n",
        "        \n",
        "        print(\"ğŸ”— Loading and merging LoRA adapter...\")\n",
        "        from peft import PeftModel\n",
        "        \n",
        "        # Load the LoRA model\n",
        "        lora_model = PeftModel.from_pretrained(base_model, final_model_path)\n",
        "        \n",
        "        # Merge the adapter\n",
        "        merged_model = lora_model.merge_and_unload()\n",
        "        \n",
        "        # Save the merged model\n",
        "        merged_model_path = f\"./{new_model_name}-merged\"\n",
        "        print(f\"ğŸ’¾ Saving merged model to: {merged_model_path}\")\n",
        "        \n",
        "        merged_model.save_pretrained(merged_model_path)\n",
        "        tokenizer.save_pretrained(merged_model_path)\n",
        "        \n",
        "        print(\"âœ… LoRA merge completed successfully!\")\n",
        "        print(f\"ğŸ“ Merged model saved to: {merged_model_path}\")\n",
        "        \n",
        "        # Create archive for merged model\n",
        "        try:\n",
        "            import shutil\n",
        "            merged_archive_name = f\"{new_model_name}-merged\"\n",
        "            shutil.make_archive(merged_archive_name, 'zip', merged_model_path)\n",
        "            archive_size = os.path.getsize(f\"{merged_archive_name}.zip\") / 1024 / 1024\n",
        "            print(f\"ğŸ“¦ Merged model archive: {merged_archive_name}.zip ({archive_size:.1f} MB)\")\n",
        "        except:\n",
        "            print(\"â„¹ï¸ Could not create merged model archive\")\n",
        "        \n",
        "        # Clean up memory\n",
        "        del base_model, lora_model, merged_model\n",
        "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "        gc.collect()\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error during LoRA merge: {e}\")\n",
        "        print(\"ğŸ’¡ The LoRA adapter is still available for use without merging\")\n",
        "        \n",
        "elif merge_lora and not can_merge:\n",
        "    print(\"âŒ Cannot merge: Insufficient memory\")\n",
        "    print(f\"ğŸ’¡ Need {required_memory}GB RAM, but only {available_memory:.1f}GB available\")\n",
        "    print(\"ğŸ”§ The LoRA adapter works fine without merging!\")\n",
        "    \n",
        "else:\n",
        "    print(\"â„¹ï¸ Skipping LoRA merge - using adapter format\")\n",
        "    print(\"ğŸ’¡ You can still use the model with the LoRA adapter!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸš€ Step 11: Upload to Hugging Face Hub (Optional)\n",
        "\n",
        "Upload your fine-tuned model to Hugging Face Hub for easy sharing and deployment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Upload to Hugging Face Hub\n",
        "upload_to_hub = input(\"ğŸ¤” Do you want to upload the model to Hugging Face Hub? (y/n): \").lower().strip() == 'y'\n",
        "\n",
        "if upload_to_hub:\n",
        "    try:\n",
        "        from huggingface_hub import HfApi, login\n",
        "        \n",
        "        print(\"ğŸ”‘ Please log in to Hugging Face Hub...\")\n",
        "        print(\"ğŸ’¡ You'll need a Hugging Face account and access token\")\n",
        "        print(\"ğŸ“ Get your token from: https://huggingface.co/settings/tokens\")\n",
        "        \n",
        "        # Login to Hugging Face\n",
        "        login()\n",
        "        \n",
        "        # Get repository name\n",
        "        repo_name = input(\"ğŸ“ Enter repository name (e.g., 'your-username/phi3-kantra-rules'): \").strip()\n",
        "        \n",
        "        if not repo_name:\n",
        "            repo_name = f\"phi3-kantra-rules-{int(time.time())}\"\n",
        "            print(f\"ğŸ·ï¸ Using default name: {repo_name}\")\n",
        "        \n",
        "        # Determine which model to upload\n",
        "        if 'merged_model_path' in locals() and os.path.exists(merged_model_path):\n",
        "            upload_path = merged_model_path\n",
        "            model_type = \"merged\"\n",
        "            print(f\"ğŸ“¤ Uploading merged model from: {upload_path}\")\n",
        "        else:\n",
        "            upload_path = final_model_path\n",
        "            model_type = \"LoRA adapter\"\n",
        "            print(f\"ğŸ“¤ Uploading LoRA adapter from: {upload_path}\")\n",
        "        \n",
        "        # Create repository and upload\n",
        "        api = HfApi()\n",
        "        \n",
        "        print(f\"ğŸ—ï¸ Creating repository: {repo_name}\")\n",
        "        api.create_repo(repo_id=repo_name, exist_ok=True)\n",
        "        \n",
        "        print(f\"ğŸ“¤ Uploading {model_type}...\")\n",
        "        api.upload_folder(\n",
        "            folder_path=upload_path,\n",
        "            repo_id=repo_name,\n",
        "            commit_message=f\"Upload fine-tuned Phi-3 mini for Kantra rules generation ({model_type})\"\n",
        "        )\n",
        "        \n",
        "        print(\"âœ… Upload completed successfully!\")\n",
        "        print(f\"ğŸ”— Your model is available at: https://huggingface.co/{repo_name}\")\n",
        "        print(f\"ğŸ’¡ You can now use it with: AutoModelForCausalLM.from_pretrained('{repo_name}')\\\")\\n\")\n",
        "        \n",
        "        # Create model card\n",
        "        model_card_content = f\\\"\\\"\\\"---\\nlicense: mit\\nbase_model: {model_id}\\ntags:\\n- phi3\\n- kantra\\n- code-migration\\n- fine-tuned\\nlibrary_name: transformers\\n---\\n\\n# Phi-3 Mini Fine-tuned for Kantra Rules Generation\\n\\nThis model is a fine-tuned version of [{model_id}](https://huggingface.co/{model_id}) for generating Kantra migration rules.\\n\\n## Model Details\\n- **Base Model**: {model_id}\\n- **Fine-tuning Method**: LoRA (Low-Rank Adaptation)\\n- **Task**: Code migration rule generation\\n- **Model Type**: {model_type}\\n\\n## Usage\\n\\n```python\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\n{'from peft import PeftModel' if model_type == 'LoRA adapter' else ''}\\n\\ntokenizer = AutoTokenizer.from_pretrained(\\\"{repo_name}\\\")\\n{'base_model = AutoModelForCausalLM.from_pretrained(\\\"' + model_id + '\\\")' if model_type == 'LoRA adapter' else ''}\\n{'model = PeftModel.from_pretrained(base_model, \\\"' + repo_name + '\\\")' if model_type == 'LoRA adapter' else 'model = AutoModelForCausalLM.from_pretrained(\\\"' + repo_name + '\\\")'}\\n\\n# Generate Kantra rules\\nprompt = \\\"Generate a Kantra rule to detect deprecated Java APIs\\\"\\nmessages = [{{\\\"role\\\": \\\"user\\\", \\\"content\\\": prompt}}]\\ninputs = tokenizer.apply_chat_template(messages, return_tensors=\\\"pt\\\")\\noutputs = model.generate(inputs, max_new_tokens=500)\\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\\nprint(response)\\n```\\n\\n## Training Details\\n- Fine-tuned using QLoRA for parameter efficiency\\n- Optimized for generating YAML-formatted Kantra migration rules\\n- Trained on custom Kantra rules dataset\\n\\\"\\\"\\\"\\n        \\n        # Upload model card\\n        with open(\\\"README.md\\\", \\\"w\\\") as f:\\n            f.write(model_card_content)\\n        \\n        api.upload_file(\\n            path_or_fileobj=\\\"README.md\\\",\\n            path_in_repo=\\\"README.md\\\",\\n            repo_id=repo_name,\\n            commit_message=\\\"Add model card\\\"\\n        )\\n        \\n        os.remove(\\\"README.md\\\")  # Clean up\\n        \\n        print(\\\"ğŸ“„ Model card created and uploaded!\\\")\\n        \\n    except ImportError:\\n        print(\\\"âŒ huggingface_hub not installed. Install with: pip install huggingface_hub\\\")\\n    except Exception as e:\\n        print(f\\\"âŒ Upload failed: {e}\\\")\\n        print(\\\"ğŸ’¡ You can manually upload the model files to Hugging Face Hub\\\")\\nelse:\\n    print(\\\"â„¹ï¸ Skipping Hugging Face Hub upload\\\")\\n    print(\\\"ğŸ’¡ You can manually upload later if needed\\\")\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸŠ Summary & Next Steps\n",
        "\n",
        "Congratulations! You've successfully fine-tuned Phi-3-mini for Kantra rules generation.\n",
        "\n",
        "### âœ… What you accomplished:\n",
        "- âœ… Fine-tuned a 3.8B parameter model using LoRA\n",
        "- âœ… Used parameter-efficient training (~1% of parameters)\n",
        "- âœ… Created a specialized model for migration rule generation\n",
        "- âœ… Handled memory constraints intelligently\n",
        "- âœ… Generated downloadable model files\n",
        "\n",
        "### ğŸ“ Your files:\n",
        "- **LoRA Adapter**: `phi-3-mini-kantra-rules-generator-final/` (always created)\n",
        "- **Merged Model**: `phi-3-mini-kantra-rules-generator-merged/` (if merged)\n",
        "- **Archives**: `.zip` files for easy download\n",
        "\n",
        "### ğŸ”„ Model Formats Explained:\n",
        "\n",
        "#### **LoRA Adapter** (Always Available):\n",
        "- âœ… **Small size**: Only adapter weights (~few MB)\n",
        "- âœ… **Memory efficient**: Works on any system\n",
        "- âœ… **Flexible**: Can be applied to different base models\n",
        "- âš ï¸ **Requires base model**: Need to load Phi-3-mini + adapter\n",
        "\n",
        "#### **Merged Model** (If you have enough RAM):\n",
        "- âœ… **Standalone**: Complete model, no base model needed\n",
        "- âœ… **Faster loading**: Single model file\n",
        "- âœ… **Easy deployment**: Standard transformer model\n",
        "- âš ï¸ **Larger size**: Full model weights (~7GB)\n",
        "\n",
        "### ğŸš€ Next steps:\n",
        "\n",
        "#### **1. Local Testing**:\n",
        "```python\n",
        "# For LoRA adapter:\n",
        "from peft import PeftModel\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
        "model = PeftModel.from_pretrained(base_model, \"./phi-3-mini-kantra-rules-generator-final\")\n",
        "\n",
        "# For merged model (if available):\n",
        "model = AutoModelForCausalLM.from_pretrained(\"./phi-3-mini-kantra-rules-generator-merged\")\n",
        "```\n",
        "\n",
        "#### **2. Production Deployment**:\n",
        "- Use merged model for faster inference\n",
        "- Use LoRA adapter for memory-constrained environments\n",
        "- Consider uploading to Hugging Face Hub for easy access\n",
        "\n",
        "#### **3. Further Improvements**:\n",
        "- ğŸ“Š Add validation dataset to monitor overfitting\n",
        "- ğŸ”§ Experiment with different LoRA ranks (8, 32, 64)\n",
        "- ğŸ“ˆ Try different learning rates (1e-4, 5e-4)\n",
        "- ğŸ“ Add more diverse training examples\n",
        "\n",
        "### ğŸ’¡ Memory Guidelines:\n",
        "- **Colab Free (12GB)**: âœ… Phi-3-mini merging works\n",
        "- **Colab Pro (25GB+)**: âœ… All models work\n",
        "- **Local Mac**: Depends on unified memory\n",
        "- **For 7B+ models**: Need 18GB+ RAM for merging\n",
        "\n",
        "---\n",
        "**ğŸ‰ Happy fine-tuning! Your Kantra rules generator is ready to use! ğŸš€**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
